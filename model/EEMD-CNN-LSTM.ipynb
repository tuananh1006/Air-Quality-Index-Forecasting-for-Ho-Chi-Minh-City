{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting EMD-signal\n",
      "  Downloading EMD_signal-1.6.4-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pathos>=0.2.1\n",
      "  Downloading pathos-0.3.4-py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.8/dist-packages (from EMD-signal) (1.4.1)\n",
      "Collecting tqdm<5.0,>=4.64.0\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.8/dist-packages (from EMD-signal) (1.21.1)\n",
      "Collecting pox>=0.3.6\n",
      "  Downloading pox-0.3.6-py3-none-any.whl (29 kB)\n",
      "Collecting dill>=0.4.0\n",
      "  Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess>=0.70.18\n",
      "  Downloading multiprocess-0.70.18-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ppft>=1.7.7\n",
      "  Downloading ppft-1.7.7-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, ppft, pox, dill, multiprocess, pathos, EMD-signal\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.42.0\n",
      "    Uninstalling tqdm-4.42.0:\n",
      "      Successfully uninstalled tqdm-4.42.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.5.1\n",
      "    Uninstalling dill-0.3.5.1:\n",
      "      Successfully uninstalled dill-0.3.5.1\n",
      "Successfully installed EMD-signal-1.6.4 dill-0.4.0 multiprocess-0.70.18 pathos-0.3.4 pox-0.3.6 ppft-1.7.7 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1752, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1390, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/segment.py\", line 245, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1368, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip'\n",
      "Call stack:\n",
      "  File \"/usr/local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/req_command.py\", line 148, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/self_outdated_check.py\", line 237, in pip_self_version_check\n",
      "    logger.info(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1446, in info\n",
      "    self._log(INFO, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.1.2', new='25.0.1'),)\n"
     ]
    }
   ],
   "source": [
    "!pip install EMD-signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ Loading Data...\n",
      "\n",
      "2Ô∏è‚É£ Running Parallel EEMD (This may take 20-40 mins)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a11c143ce484db3a46c9723f33f0a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EEMD Progress:   0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Merging chunks...\n",
      "‚úÖ Added 9 IMF columns to dataset.\n",
      "\n",
      "3Ô∏è‚É£ Feature Engineering...\n",
      "5Ô∏è‚É£ Scaling Data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_squared_log_error\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from PyEMD import EEMD\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ===============================\n",
    "# 1Ô∏è‚É£ Load Data & Basic Preprocessing\n",
    "# ===============================\n",
    "print(\"1Ô∏è‚É£ Loading Data...\")\n",
    "df = pd.read_csv(\"full_data_imputed.csv\", index_col=False)\n",
    "\n",
    "# Clean up columns\n",
    "drop_cols = ['Unnamed: 0', 'Latitude', 'Longitude']\n",
    "df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)\n",
    "\n",
    "df_copy = df.copy()\n",
    "df_copy['Local Time'] = pd.to_datetime(df_copy['Local Time'])\n",
    "df_copy = df_copy.set_index('Local Time')\n",
    "\n",
    "# Fill missing values\n",
    "weather_cols = ['Clouds', 'Precipitation', 'Pressure', 'Relative Humidity', 'Temperature', 'UV_Index', 'Wind Speed']\n",
    "existing_cols = [col for col in weather_cols if col in df_copy.columns]\n",
    "df_copy[existing_cols] = df_copy[existing_cols].ffill().bfill()\n",
    "\n",
    "target_col = 'Aqi'\n",
    "\n",
    "# ===============================\n",
    "# 2Ô∏è‚É£ EEMD Preprocessing (TURBO MODE: Parallel + Chunking)\n",
    "# ===============================\n",
    "print(\"\\n2Ô∏è‚É£ Running Parallel EEMD (This may take 20-40 mins)...\")\n",
    "\n",
    "# --- C·∫•u h√¨nh ---\n",
    "trials = 50       # S·ªë l·∫ßn th·ª≠ (Gi·∫£m t·ª´ 100 xu·ªëng 50 ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô g·∫•p ƒë√¥i)\n",
    "noise_width = 0.2 \n",
    "K_target = 9      # Gi·ªØ l·∫°i ƒë√∫ng 9 modes\n",
    "CHUNK_SIZE = 2000 # C·∫Øt nh·ªè d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω song song\n",
    "\n",
    "signal = df_copy[target_col].values\n",
    "n_samples = len(signal)\n",
    "\n",
    "# --- H√†m x·ª≠ l√Ω 1 chunk (Ch·∫°y tr√™n 1 core CPU) ---\n",
    "def process_single_chunk(chunk_idx, start_idx, end_idx, signal_chunk, trials, noise, k_target):\n",
    "    try:\n",
    "        # Init EEMD\n",
    "        eemd = EEMD(trials=trials, noise_width=noise)\n",
    "        # Run EEMD\n",
    "        imfs = eemd.eemd(signal_chunk)\n",
    "        \n",
    "        # --- Fix Shape & Modes ---\n",
    "        current_k, length = imfs.shape\n",
    "        result_matrix = np.zeros((k_target, length))\n",
    "        \n",
    "        if current_k >= k_target:\n",
    "            result_matrix = imfs[:k_target, :]\n",
    "        else:\n",
    "            result_matrix[:current_k, :] = imfs\n",
    "            \n",
    "        return (chunk_idx, result_matrix.T)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Return zeros on error to prevent crash\n",
    "        return (chunk_idx, np.zeros((end_idx - start_idx, k_target)))\n",
    "\n",
    "# --- T·∫°o danh s√°ch tasks ---\n",
    "tasks = []\n",
    "for i in range(0, n_samples, CHUNK_SIZE):\n",
    "    end_idx = min(i + CHUNK_SIZE, n_samples)\n",
    "    chunk_signal = signal[i:end_idx]\n",
    "    if len(chunk_signal) > 50: # B·ªè qua ƒëo·∫°n qu√° ng·∫Øn\n",
    "        tasks.append((i // CHUNK_SIZE, i, end_idx, chunk_signal))\n",
    "\n",
    "# --- Ch·∫°y song song (n_jobs=-1: D√πng t·∫•t c·∫£ CPU) ---\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(process_single_chunk)(idx, start, end, sig, trials, noise_width, K_target)\n",
    "    for idx, start, end, sig in tqdm(tasks, desc=\"EEMD Progress\")\n",
    ")\n",
    "\n",
    "# --- Gh√©p d·ªØ li·ªáu ---\n",
    "print(\"   -> Merging chunks...\")\n",
    "results.sort(key=lambda x: x[0])\n",
    "final_imfs = np.concatenate([res[1] for res in results], axis=0)\n",
    "\n",
    "# Padding n·∫øu thi·∫øu d√≤ng cu·ªëi\n",
    "if len(final_imfs) < len(df_copy):\n",
    "    missing = len(df_copy) - len(final_imfs)\n",
    "    padding = np.zeros((missing, K_target))\n",
    "    final_imfs = np.vstack([final_imfs, padding])\n",
    "\n",
    "# Th√™m v√†o DataFrame\n",
    "for i in range(K_target):\n",
    "    df_copy[f'IMF_{i+1}'] = final_imfs[:, i]\n",
    "\n",
    "print(f\"‚úÖ Added {K_target} IMF columns to dataset.\")\n",
    "\n",
    "# ===============================\n",
    "# 3Ô∏è‚É£ Feature Engineering\n",
    "# ===============================\n",
    "print(\"\\n3Ô∏è‚É£ Feature Engineering...\")\n",
    "df_copy['Month'] = df_copy.index.month\n",
    "df_copy['Day'] = df_copy.index.day\n",
    "df_copy['Hour'] = df_copy.index.hour\n",
    "df_copy['Weekday'] = df_copy.index.weekday\n",
    "\n",
    "# ===============================\n",
    "# 4Ô∏è‚É£ Split Data\n",
    "# ===============================\n",
    "train_data = df_copy.loc['2023':'2024']\n",
    "val_data = df_copy.loc['2025-01':'2025-06']\n",
    "test_data = df_copy.loc['2025-07':'2025-09']\n",
    "\n",
    "X_train, y_train = train_data.drop(columns=[target_col]), train_data[target_col]\n",
    "X_val, y_val = val_data.drop(columns=[target_col]), val_data[target_col]\n",
    "X_test, y_test = test_data.drop(columns=[target_col]), test_data[target_col]\n",
    "\n",
    "# ===============================\n",
    "# 5Ô∏è‚É£ Scaling\n",
    "# ===============================\n",
    "print(\"5Ô∏è‚É£ Scaling Data...\")\n",
    "scaler_X = StandardScaler()\n",
    "# L∆∞u √Ω: X ƒë√£ bao g·ªìm c√°c c·ªôt IMF n√™n ch√∫ng c≈©ng ƒë∆∞·ª£c scale\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_val_scaled = scaler_X.transform(X_val)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# ===============================\n",
    "# 6Ô∏è‚É£ Create Datasets\n",
    "# ===============================\n",
    "length = 24  # 24 hours window\n",
    "batch_size = 32\n",
    "n_features = X_train_scaled.shape[1]\n",
    "\n",
    "train_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "    X_train_scaled, targets=y_train_scaled, sequence_length=length, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "    X_val_scaled, targets=y_val_scaled, sequence_length=length, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "    X_test_scaled, targets=y_test_scaled, sequence_length=length, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ===============================\n",
    "# 7Ô∏è‚É£ Model Definition\n",
    "# ===============================\n",
    "def rmsle_custom(y_true, y_pred):\n",
    "    msle = tf.keras.losses.MeanSquaredLogarithmicError()\n",
    "    return tf.sqrt(msle(y_true, y_pred))\n",
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "    ss_res = K.sum(K.square(y_true - y_pred))\n",
    "    ss_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return 1 - ss_res / (ss_tot + K.epsilon())\n",
    "\n",
    "model = Sequential()\n",
    "# CNN Layers\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(length, n_features)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "# LSTM Layers\n",
    "model.add(LSTM(units=100, return_sequences=True, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50, return_sequences=False, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "# Output\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=[rmsle_custom, 'mae', 'mse', tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mape', 'msle', r2_keras]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8Ô∏è‚É£ Training Model...\n",
      "Epoch 1/50\n",
      "21421/21421 [==============================] - 143s 6ms/step - loss: 0.0387 - rmsle_custom: 0.0324 - mae: 0.1179 - mse: 0.0387 - rmse: 0.1966 - mape: 76.1702 - msle: 0.0055 - r2_keras: -1.4181 - val_loss: 0.0275 - val_rmsle_custom: 0.0439 - val_mae: 0.1250 - val_mse: 0.0275 - val_rmse: 0.1657 - val_mape: 98.1339 - val_msle: 0.0050 - val_r2_keras: -0.1070 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "21421/21421 [==============================] - 139s 7ms/step - loss: 0.0180 - rmsle_custom: 0.0225 - mae: 0.0844 - mse: 0.0180 - rmse: 0.1341 - mape: 47.0983 - msle: 0.0018 - r2_keras: -0.5977 - val_loss: 0.0194 - val_rmsle_custom: 0.0344 - val_mae: 0.1053 - val_mse: 0.0194 - val_rmse: 0.1391 - val_mape: 74.1165 - val_msle: 0.0030 - val_r2_keras: 0.0417 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "21421/21421 [==============================] - 139s 7ms/step - loss: 0.0162 - rmsle_custom: 0.0209 - mae: 0.0799 - mse: 0.0162 - rmse: 0.1273 - mape: 43.3626 - msle: 0.0015 - r2_keras: 0.1271 - val_loss: 0.0158 - val_rmsle_custom: 0.0306 - val_mae: 0.0957 - val_mse: 0.0158 - val_rmse: 0.1259 - val_mape: 62.8562 - val_msle: 0.0023 - val_r2_keras: 0.1246 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "21421/21421 [==============================] - 140s 7ms/step - loss: 0.0143 - rmsle_custom: 0.0196 - mae: 0.0759 - mse: 0.0143 - rmse: 0.1197 - mape: 41.5870 - msle: 0.0014 - r2_keras: 0.2560 - val_loss: 0.0126 - val_rmsle_custom: 0.0266 - val_mae: 0.0863 - val_mse: 0.0126 - val_rmse: 0.1122 - val_mape: 66.1184 - val_msle: 0.0017 - val_r2_keras: 0.2699 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "21421/21421 [==============================] - 140s 7ms/step - loss: 0.0140 - rmsle_custom: 0.0194 - mae: 0.0731 - mse: 0.0140 - rmse: 0.1181 - mape: 41.0352 - msle: 0.0014 - r2_keras: -0.1085 - val_loss: 0.0123 - val_rmsle_custom: 0.0296 - val_mae: 0.0839 - val_mse: 0.0123 - val_rmse: 0.1110 - val_mape: 62.8586 - val_msle: 0.0021 - val_r2_keras: 0.3520 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "21421/21421 [==============================] - 140s 7ms/step - loss: 0.0122 - rmsle_custom: 0.0185 - mae: 0.0699 - mse: 0.0122 - rmse: 0.1106 - mape: 39.1624 - msle: 0.0012 - r2_keras: 0.2742 - val_loss: 0.0123 - val_rmsle_custom: 0.0266 - val_mae: 0.0854 - val_mse: 0.0123 - val_rmse: 0.1108 - val_mape: 66.3976 - val_msle: 0.0017 - val_r2_keras: 0.3106 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "21421/21421 [==============================] - 140s 7ms/step - loss: 0.0119 - rmsle_custom: 0.0183 - mae: 0.0688 - mse: 0.0119 - rmse: 0.1093 - mape: 39.2012 - msle: 0.0011 - r2_keras: -0.6133 - val_loss: 0.0126 - val_rmsle_custom: 0.0272 - val_mae: 0.0854 - val_mse: 0.0126 - val_rmse: 0.1122 - val_mape: 64.0332 - val_msle: 0.0019 - val_r2_keras: 0.3049 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "21421/21421 [==============================] - 140s 7ms/step - loss: 0.0116 - rmsle_custom: 0.0181 - mae: 0.0677 - mse: 0.0116 - rmse: 0.1076 - mape: 38.4843 - msle: 0.0011 - r2_keras: -1.2276 - val_loss: 0.0111 - val_rmsle_custom: 0.0268 - val_mae: 0.0795 - val_mse: 0.0111 - val_rmse: 0.1052 - val_mape: 60.7080 - val_msle: 0.0019 - val_r2_keras: 0.3942 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "21421/21421 [==============================] - 140s 7ms/step - loss: 0.0114 - rmsle_custom: 0.0179 - mae: 0.0670 - mse: 0.0114 - rmse: 0.1068 - mape: 38.4480 - msle: 0.0011 - r2_keras: -1.2015 - val_loss: 0.0128 - val_rmsle_custom: 0.0289 - val_mae: 0.0856 - val_mse: 0.0128 - val_rmse: 0.1133 - val_mape: 63.0282 - val_msle: 0.0023 - val_r2_keras: 0.3329 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "21421/21421 [==============================] - 140s 7ms/step - loss: 0.0110 - rmsle_custom: 0.0177 - mae: 0.0663 - mse: 0.0110 - rmse: 0.1050 - mape: 37.6507 - msle: 0.0010 - r2_keras: 0.4057 - val_loss: 0.0111 - val_rmsle_custom: 0.0252 - val_mae: 0.0808 - val_mse: 0.0111 - val_rmse: 0.1053 - val_mape: 60.9524 - val_msle: 0.0016 - val_r2_keras: 0.3777 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "21421/21421 [==============================] - ETA: 0s - loss: 0.0107 - rmsle_custom: 0.0175 - mae: 0.0654 - mse: 0.0107 - rmse: 0.1034 - mape: 37.0965 - msle: 9.9549e-04 - r2_keras: -0.9066\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "21421/21421 [==============================] - 139s 7ms/step - loss: 0.0107 - rmsle_custom: 0.0175 - mae: 0.0654 - mse: 0.0107 - rmse: 0.1034 - mape: 37.0965 - msle: 9.9549e-04 - r2_keras: -0.9066 - val_loss: 0.0125 - val_rmsle_custom: 0.0260 - val_mae: 0.0862 - val_mse: 0.0125 - val_rmse: 0.1116 - val_mape: 66.0545 - val_msle: 0.0016 - val_r2_keras: 0.2931 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "21421/21421 [==============================] - 140s 7ms/step - loss: 0.0105 - rmsle_custom: 0.0169 - mae: 0.0648 - mse: 0.0105 - rmse: 0.1023 - mape: 36.6278 - msle: 9.3262e-04 - r2_keras: -4.2213 - val_loss: 0.0116 - val_rmsle_custom: 0.0260 - val_mae: 0.0840 - val_mse: 0.0116 - val_rmse: 0.1075 - val_mape: 58.7815 - val_msle: 0.0016 - val_r2_keras: 0.3845 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "21421/21421 [==============================] - 140s 7ms/step - loss: 0.0096 - rmsle_custom: 0.0165 - mae: 0.0624 - mse: 0.0096 - rmse: 0.0978 - mape: 35.0041 - msle: 8.4998e-04 - r2_keras: -4.0727 - val_loss: 0.0109 - val_rmsle_custom: 0.0262 - val_mae: 0.0807 - val_mse: 0.0109 - val_rmse: 0.1046 - val_mape: 57.1760 - val_msle: 0.0017 - val_r2_keras: 0.4350 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "21421/21421 [==============================] - 140s 7ms/step - loss: 0.0091 - rmsle_custom: 0.0161 - mae: 0.0609 - mse: 0.0091 - rmse: 0.0955 - mape: 34.1005 - msle: 8.1116e-04 - r2_keras: -0.6378 - val_loss: 0.0125 - val_rmsle_custom: 0.0273 - val_mae: 0.0863 - val_mse: 0.0125 - val_rmse: 0.1119 - val_mape: 56.0470 - val_msle: 0.0018 - val_r2_keras: 0.3593 - lr: 5.0000e-04\n",
      "Epoch 15/50\n",
      "21421/21421 [==============================] - 140s 7ms/step - loss: 0.0090 - rmsle_custom: 0.0159 - mae: 0.0604 - mse: 0.0090 - rmse: 0.0950 - mape: 33.5258 - msle: 7.8751e-04 - r2_keras: 0.1612 - val_loss: 0.0128 - val_rmsle_custom: 0.0248 - val_mae: 0.0886 - val_mse: 0.0128 - val_rmse: 0.1131 - val_mape: 57.4061 - val_msle: 0.0015 - val_r2_keras: 0.3054 - lr: 5.0000e-04\n",
      "Epoch 16/50\n",
      "21417/21421 [============================>.] - ETA: 0s - loss: 0.0088 - rmsle_custom: 0.0157 - mae: 0.0596 - mse: 0.0088 - rmse: 0.0938 - mape: 32.9687 - msle: 7.5683e-04 - r2_keras: 0.5814\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "21421/21421 [==============================] - 140s 7ms/step - loss: 0.0088 - rmsle_custom: 0.0157 - mae: 0.0596 - mse: 0.0088 - rmse: 0.0938 - mape: 32.9732 - msle: 7.5686e-04 - r2_keras: -3.3056 - val_loss: 0.0141 - val_rmsle_custom: 0.0267 - val_mae: 0.0947 - val_mse: 0.0141 - val_rmse: 0.1187 - val_mape: 55.5766 - val_msle: 0.0018 - val_r2_keras: 0.2006 - lr: 5.0000e-04\n",
      "Epoch 17/50\n",
      "21421/21421 [==============================] - 140s 7ms/step - loss: 0.0088 - rmsle_custom: 0.0156 - mae: 0.0600 - mse: 0.0088 - rmse: 0.0939 - mape: 33.4883 - msle: 7.4953e-04 - r2_keras: 0.5732 - val_loss: 0.0124 - val_rmsle_custom: 0.0222 - val_mae: 0.0898 - val_mse: 0.0124 - val_rmse: 0.1112 - val_mape: 56.2195 - val_msle: 0.0012 - val_r2_keras: 0.2789 - lr: 2.5000e-04\n",
      "Epoch 18/50\n",
      "21421/21421 [==============================] - 140s 7ms/step - loss: 0.0084 - rmsle_custom: 0.0153 - mae: 0.0586 - mse: 0.0084 - rmse: 0.0919 - mape: 32.1637 - msle: 7.1280e-04 - r2_keras: -0.6800 - val_loss: 0.0121 - val_rmsle_custom: 0.0219 - val_mae: 0.0883 - val_mse: 0.0121 - val_rmse: 0.1102 - val_mape: 53.3459 - val_msle: 0.0012 - val_r2_keras: 0.3084 - lr: 2.5000e-04\n",
      "Epoch 19/50\n",
      "21416/21421 [============================>.] - ETA: 0s - loss: 0.0084 - rmsle_custom: 0.0151 - mae: 0.0580 - mse: 0.0084 - rmse: 0.0914 - mape: 31.3533 - msle: 6.9552e-04 - r2_keras: 0.6092\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "21421/21421 [==============================] - 140s 7ms/step - loss: 0.0084 - rmsle_custom: 0.0151 - mae: 0.0580 - mse: 0.0084 - rmse: 0.0914 - mape: 31.3557 - msle: 6.9550e-04 - r2_keras: 0.5949 - val_loss: 0.0127 - val_rmsle_custom: 0.0220 - val_mae: 0.0906 - val_mse: 0.0127 - val_rmse: 0.1127 - val_mape: 53.6496 - val_msle: 0.0012 - val_r2_keras: 0.2690 - lr: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 8Ô∏è‚É£ Training\n",
    "# ===============================\n",
    "print(\"\\n8Ô∏è‚É£ Training Model...\")\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1, min_lr=1e-6)\n",
    "\n",
    "hist = model.fit(train_dataset, validation_data=val_dataset, epochs=50, callbacks=[es, plateau], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------\n",
      "ƒêang t√≠nh to√°n Full Metrics tr√™n t·∫≠p Test...\n",
      "------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "üìä REPORT ƒê√ÅNH GI√Å M√î H√åNH VMD-CNN-LSTM\n",
      "==================================================\n",
      "Metric     | Gi√° tr·ªã         | √ù nghƒ©a ng·∫Øn g·ªçn\n",
      "--------------------------------------------------\n",
      "MAE        | 13.0486         | Sai s·ªë tuy·ªát ƒë·ªëi trung b√¨nh (ƒë∆°n v·ªã AQI)\n",
      "MSE        | 254.6857        | B√¨nh ph∆∞∆°ng sai s·ªë (ph·∫°t l·ªói l·ªõn n·∫∑ng h∆°n)\n",
      "RMSE       | 15.9589         | Sai s·ªë chu·∫©n (c√πng ƒë∆°n v·ªã v·ªõi AQI)\n",
      "MSLE       | 0.1964          | Sai s·ªë logarit b√¨nh ph∆∞∆°ng\n",
      "RMSLE      | 0.4432          | Sai s·ªë logarit (t·ªët n·∫øu d·ªØ li·ªáu ch√™nh l·ªách l·ªõn)\n",
      "MAPE       | 48.78          % | Sai s·ªë theo ph·∫ßn trƒÉm\n",
      "R¬≤         | 0.7310          | ƒê·ªô ph√π h·ª£p (Max = 1.0)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_squared_log_error\n",
    "\n",
    "# ===============================\n",
    "# üîü D·ª± ƒëo√°n & Inverse Transform\n",
    "# ===============================\n",
    "print(\"\\n------------------------------------------------\")\n",
    "print(\"ƒêang t√≠nh to√°n Full Metrics tr√™n t·∫≠p Test...\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# 1. D·ª± ƒëo√°n (Output ƒëang ·ªü d·∫°ng Scaled)\n",
    "y_pred_scaled = model.predict(test_dataset, verbose=0)\n",
    "\n",
    "# 2. L·∫•y y_true (Output ƒëang ·ªü d·∫°ng Scaled) t·ª´ dataset\n",
    "y_true_scaled = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "\n",
    "# 3. ƒê∆∞a v·ªÅ gi√° tr·ªã g·ªëc (Original Scale)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_true = scaler_y.inverse_transform(y_true_scaled)\n",
    "\n",
    "# 4. X·ª≠ l√Ω gi√° tr·ªã √¢m (Quan tr·ªçng cho MSLE/RMSLE)\n",
    "# AQI kh√¥ng th·ªÉ √¢m, nh∆∞ng model c√≥ th·ªÉ d·ª± ƒëo√°n ra s·ªë √¢m nh·ªè.\n",
    "# Ta c·∫ßn ƒë∆∞a v·ªÅ 0 ƒë·ªÉ tr√°nh l·ªói logarit.\n",
    "y_pred_safe = np.maximum(y_pred, 0)\n",
    "y_true_safe = np.maximum(y_true, 0)\n",
    "\n",
    "# ===============================\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ T√≠nh to√°n 7 Metrics\n",
    "# ===============================\n",
    "\n",
    "# 1. MAE\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# 2. MSE\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# 3. RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# 4. MSLE (Mean Squared Logarithmic Error)\n",
    "msle = mean_squared_log_error(y_true_safe, y_pred_safe)\n",
    "\n",
    "# 5. RMSLE (Root Mean Squared Logarithmic Error)\n",
    "rmsle = np.sqrt(msle)\n",
    "\n",
    "# 6. MAPE (Mean Absolute Percentage Error)\n",
    "# Th√™m epsilon (1e-7) ƒë·ªÉ tr√°nh chia cho 0\n",
    "mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-7))) * 100\n",
    "\n",
    "# 7. R-squared\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# ===============================\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ In k·∫øt qu·∫£ Report\n",
    "# ===============================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"üìä REPORT ƒê√ÅNH GI√Å M√î H√åNH VMD-CNN-LSTM\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Metric':<10} | {'Gi√° tr·ªã':<15} | {'√ù nghƒ©a ng·∫Øn g·ªçn'}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'MAE':<10} | {mae:<15.4f} | Sai s·ªë tuy·ªát ƒë·ªëi trung b√¨nh (ƒë∆°n v·ªã AQI)\")\n",
    "print(f\"{'MSE':<10} | {mse:<15.4f} | B√¨nh ph∆∞∆°ng sai s·ªë (ph·∫°t l·ªói l·ªõn n·∫∑ng h∆°n)\")\n",
    "print(f\"{'RMSE':<10} | {rmse:<15.4f} | Sai s·ªë chu·∫©n (c√πng ƒë∆°n v·ªã v·ªõi AQI)\")\n",
    "print(f\"{'MSLE':<10} | {msle:<15.4f} | Sai s·ªë logarit b√¨nh ph∆∞∆°ng\")\n",
    "print(f\"{'RMSLE':<10} | {rmsle:<15.4f} | Sai s·ªë logarit (t·ªët n·∫øu d·ªØ li·ªáu ch√™nh l·ªách l·ªõn)\")\n",
    "print(f\"{'MAPE':<10} | {mape:<15.2f}% | Sai s·ªë theo ph·∫ßn trƒÉm\")\n",
    "print(f\"{'R¬≤':<10} | {r2:<15.4f} | ƒê·ªô ph√π h·ª£p (Max = 1.0)\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the multi-class problem can be solved near perfectly with XGBoost.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we saw in the binary and multi-class classification problems, XGBoost can be very effective at detecting anomalies when you have labeled data. In labs 2 and 3, we will consider the same KDD99 dataset but we will train deep learning models to detect anomalies without using the labels.  This will mimic a more likely situation is the real world.\n",
    "\n",
    "- GPU-Accelerating XGboost through RAPIDS is easy and fast.  The only change we had to make to use the GPU was to set the 'tree_method' parameter to 'gpu_hist'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "<li>\n",
    "Dhaliwal, S., Nahid, A., & Abbas, R. (2018). Effective Intrusion Detection System Using XGBoost. Information, 9(7), 149. doi:10.3390/info9070149\n",
    "</li>\n",
    "<li>\n",
    "Brownlee, J. A Gentle Introduction to XGBoost for Applied Machine Learning. Machine Learning\n",
    "Mastery. Available online: http://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/\n",
    "(accessed on 2 March 2018).\n",
    "    </li>\n",
    "    <li>A Study on NSL-KDD Dataset for Intrusion Detection System Based on Classification Algorithms. Available\n",
    "online: https://pdfs.semanticscholar.org/1b34/80021c4ab0f632efa99e01a9b073903c5554.pdf (accessed on\n",
    "        26 March 2018)</li>\n",
    "    <li>\n",
    "        XGBoost Parameters‚ÄîXgboost 0.7 Documentation. Available online: http://xgboost.readthedocs.io/en/\n",
    "latest/parameter.html (accessed on 12 March 2018)\n",
    "    </li>\n",
    "    <li>\n",
    "        RAPIDS Documentation and Cheat Sheet.Available online: https://rapids.ai/documentation.html\n",
    "    </li>\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/deep-learning-ai/education/\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
